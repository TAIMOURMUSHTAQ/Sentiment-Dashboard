{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f1919a0",
        "outputId": "61bfed0a-6b72-45ab-85ef-8b299b89ee8b"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\"\"\"\n",
        "Sentiment Analysis Dashboard (Streamlit)\n",
        "Fast baseline (TF-IDF + LogisticRegression\n",
        "CSV upload & batch prediction\n",
        "Manual text input for single prediction\n",
        "Visualizations (pie, bar, confusion matrix)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import plotly.express as px\n",
        "\n",
        "# Text cleaning\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Caching and warnings\n",
        "from functools import partial\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk_downloader_done = False\n",
        "try:\n",
        "    stopwords.words(\"english\")\n",
        "    nltk_downloader_done = True\n",
        "except Exception:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk_downloader_done = True\n",
        "\n",
        "EN_STOPS = set(stopwords.words(\"english\"))\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "\n",
        "#Helper functions\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Basic cleaning: lowercase, remove URLs, punctuation, stopwords, and stem.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # remove urls\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)  # letters only\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in EN_STOPS and len(t) > 1]\n",
        "    stems = [STEMMER.stem(t) for t in tokens]\n",
        "    return \" \".join(stems)\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def preprocess_series(series: pd.Series) -> pd.Series:\n",
        "    return series.fillna(\"\").astype(str).apply(clean_text)\n",
        "\n",
        "# Fast baseline training pipeline (TF-IDF + LR)\n",
        "@st.cache_resource\n",
        "def train_baseline(X_texts, y_labels):\n",
        "    \"\"\"Train and return a pipeline and vectorizer+model for quick inference.\"\"\"\n",
        "    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "    clf = LogisticRegression(max_iter=200, C=1.0, solver='liblinear')\n",
        "    pipeline = Pipeline([\n",
        "        (\"tfidf\", tfidf),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "    pipeline.fit(X_texts, y_labels)\n",
        "    return pipeline\n",
        "\n",
        "# Predict helper\n",
        "def predict_df(pipeline, texts: pd.Series):\n",
        "    preds = pipeline.predict(texts)\n",
        "    probs = pipeline.predict_proba(texts) if hasattr(pipeline, \"predict_proba\") else None\n",
        "    return preds, probs\n",
        "\n",
        "# Utility: metrics summary\n",
        "def get_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Save & load baseline model (joblib)\n",
        "MODEL_PATH = \"baseline_sentiment_model.joblib\"\n",
        "def save_model(pipeline, path=MODEL_PATH):\n",
        "    joblib.dump(pipeline, path)\n",
        "\n",
        "def load_model(path=MODEL_PATH):\n",
        "    if os.path.exists(path):\n",
        "        return joblib.load(path)\n",
        "    return None\n",
        "\n",
        "# Streamlit UI\n",
        "\n",
        "st.set_page_config(page_title=\"Sentiment Analysis Dashboard\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
        "st.title(\"üìù Sentiment Analysis Dashboard\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    Upload a CSV containing text and optionally labels (e.g. 'text' and 'label' columns).\n",
        "    Quick baseline model (TFIDF + Logistic Regression) is trained instantly.\n",
        "    Toggle the *Advanced* panel to fine-tune DistilBERT (requires GPU).\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Sidebar controls\n",
        "st.sidebar.header(\"Configuration\")\n",
        "text_column = st.sidebar.text_input(\"Text column name (CSV)\", value=\"text\")\n",
        "label_column = st.sidebar.text_input(\"Label column name (CSV, optional)\", value=\"label (optional)\")\n",
        "train_fraction = st.sidebar.slider(\"Train fraction (if labels present)\", 0.1, 0.9, 0.8)\n",
        "\n",
        "# Fine-tune options\n",
        "st.sidebar.markdown(\"---\")\n",
        "use_distilbert = st.sidebar.checkbox(\"Enable DistilBERT fine-tuning (advanced)\", value=False)\n",
        "if use_distilbert:\n",
        "    st.sidebar.info(\"DistilBERT requires 'transformers', 'datasets' and ideally a GPU. Use cautiously.\")\n",
        "    bert_epochs = st.sidebar.number_input(\"Epochs\", min_value=1, max_value=10, value=2)\n",
        "    bert_batch = st.sidebar.selectbox(\"Batch size\", [8, 16, 32], index=1)\n",
        "    bert_lr = st.sidebar.number_input(\"Learning rate\", 1e-6, 1e-3, value=2e-5, format=\"%.6f\")\n",
        "\n",
        "# Upload & sample\n",
        "st.subheader(\"1) Upload data (CSV) or use sample\")\n",
        "uploaded_file = st.file_uploader(\"Upload CSV file (must include text column)\", type=[\"csv\"])\n",
        "sample_button, sample_col = st.columns([1,3])\n",
        "with sample_col:\n",
        "    st.markdown(\"Or use a small sample dataset (IMDB subset) for demo:\")\n",
        "\n",
        "use_sample = st.button(\"Load Demo Sample (small IMDB)\")\n",
        "\n",
        "if use_sample:\n",
        "    # small demo dataset created inline (so there's no external dependency)\n",
        "    demo_texts = [\n",
        "        \"I loved this movie, it was fantastic and thrilling!\",\n",
        "        \"Horrible experience. The product broke on day one.\",\n",
        "        \"It was okay, not the best but not the worst.\",\n",
        "        \"Excellent service and very friendly staff.\",\n",
        "        \"Terrible. Will never buy again.\"\n",
        "    ]\n",
        "    demo_labels = [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\"]\n",
        "    df = pd.DataFrame({\"text\": demo_texts, \"label\": demo_labels})\n",
        "else:\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Failed to read CSV: {e}\")\n",
        "            st.stop()\n",
        "    else:\n",
        "        # empty state\n",
        "        st.info(\"Upload a CSV or press 'Load Demo Sample' to try the app.\")\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    st.write(\"Dataset preview:\")\n",
        "    st.dataframe(df.head(10))\n",
        "\n",
        "    # Validate text column\n",
        "    if text_column not in df.columns:\n",
        "        # try to auto-detect a text-like column\n",
        "        text_candidates = [c for c in df.columns if df[c].dtype == object or df[c].dtype == 'string']\n",
        "        if text_candidates:\n",
        "            detected = st.selectbox(\"Detected text columns - choose one\", options=text_candidates)\n",
        "            text_column = detected\n",
        "            st.sidebar.text_input(\"Text column name (CSV)\", value=text_column)\n",
        "        else:\n",
        "            st.error(f\"Could not find a text column named '{text_column}'. Please upload valid CSV.\")\n",
        "            st.stop()\n",
        "\n",
        "    texts_raw = df[text_column].astype(str).fillna(\"\")\n",
        "\n",
        "    # If label column exists and is in df\n",
        "    y = None\n",
        "    has_labels = False\n",
        "    if label_column in df.columns:\n",
        "        y = df[label_column].astype(str).fillna(\"\")\n",
        "        has_labels = True\n",
        "    else:\n",
        "        # If user provided a label name but not present, warn; else proceed unlabeled.\n",
        "        if label_column.strip() and label_column != \"label (optional)\":\n",
        "            st.warning(f\"Label column '{label_column}' not found ‚Äî running in unlabeled mode.\")\n",
        "        has_labels = False\n",
        "\n",
        "    # Preprocess texts\n",
        "    with st.spinner(\"Cleaning text (cached)...\"):\n",
        "        texts_clean = preprocess_series(texts_raw)\n",
        "\n",
        "    # If labels present, prepare and show distribution\n",
        "    if has_labels:\n",
        "        st.subheader(\"Label distribution\")\n",
        "        label_counts = y.value_counts().rename_axis('label').reset_index(name='counts')\n",
        "        fig = px.pie(label_counts, names='label', values='counts', title='Label distribution')\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    # If labeled we can train/test baseline\n",
        "    if has_labels and st.button(\"Train baseline model (TF-IDF + LogisticRegression)\"):\n",
        "        # split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts_clean, y, train_size=train_fraction, random_state=42, stratify=y if has_labels else None\n",
        "        )\n",
        "        with st.spinner(\"Training baseline (cached) ...\"):\n",
        "            baseline_pipeline = train_baseline(X_train, y_train)\n",
        "            # Save model\n",
        "            save_model(baseline_pipeline)\n",
        "        st.success(\"Baseline trained and cached!\")\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = baseline_pipeline.predict(X_test)\n",
        "        metrics = get_metrics(y_test, y_pred)\n",
        "        st.subheader(\"Baseline performance (on test set)\")\n",
        "        st.write(metrics)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))\n",
        "        fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test), ax=ax2)\n",
        "        ax2.set_xlabel(\"Predicted\")\n",
        "        ax2.set_ylabel(\"Actual\")\n",
        "        ax2.set_title(\"Confusion Matrix\")\n",
        "        st.pyplot(fig2)\n",
        "\n",
        "        st.subheader(\"Classification report\")\n",
        "        st.text(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    # If user wants to load existing baseline model\n",
        "    if st.button(\"Load saved baseline model (if exists)\"):\n",
        "        loaded = load_model()\n",
        "        if loaded is not None:\n",
        "            st.success(\"Loaded saved baseline model.\")\n",
        "            baseline_pipeline = loaded\n",
        "        else:\n",
        "            st.warning(\"No saved baseline model found. Train one first.\")\n",
        "            baseline_pipeline = None\n",
        "\n",
        "    # Single-text prediction UI\n",
        "    st.subheader(\"Single Text Prediction\")\n",
        "    input_text = st.text_area(\"Type text here for live prediction\", height=120)\n",
        "    predict_button = st.button(\"Predict this text\")\n",
        "    if predict_button:\n",
        "        if not input_text.strip():\n",
        "            st.warning(\"Please enter text to predict.\")\n",
        "        else:\n",
        "            # prefer using loaded/trained pipeline if available, else quick train on all data if labels present\n",
        "            pipeline = load_model()\n",
        "            if pipeline is None:\n",
        "                # if labels are present, train quickly on whole dataset; else train on demo (no labels -> can't)\n",
        "                if has_labels:\n",
        "                    with st.spinner(\"Training quick baseline on all labeled data...\"):\n",
        "                        pipeline = train_baseline(texts_clean, y)\n",
        "                        save_model(pipeline)\n",
        "                else:\n",
        "                    st.error(\"No trained model available and no labels to train on.\")\n",
        "                    pipeline = None\n",
        "            if pipeline is not None:\n",
        "                cleaned = clean_text(input_text)\n",
        "                pred = pipeline.predict([cleaned])[0]\n",
        "                proba = pipeline.predict_proba([cleaned])[0] if hasattr(pipeline, \"predict_proba\") else None\n",
        "                st.write(f\"**Predicted label:** {pred}\")\n",
        "                if proba is not None:\n",
        "                    probs_df = pd.DataFrame([proba], columns=pipeline.classes_)\n",
        "                    st.write(\"**Probabilities:**\")\n",
        "                    st.dataframe(probs_df.T.rename(columns={0:\"probability\"}))\n",
        "\n",
        "    # Batch prediction & download\n",
        "    st.subheader(\"Batch prediction & download\")\n",
        "    if st.button(\"Run batch prediction on uploaded data\"):\n",
        "        pipeline = load_model()\n",
        "        if pipeline is None:\n",
        "            st.warning(\"No saved baseline model found. Training baseline quickly on all labeled data (if labels exist)...\")\n",
        "            if has_labels:\n",
        "                with st.spinner(\"Training baseline on all labeled data...\"):\n",
        "                    pipeline = train_baseline(texts_clean, y)\n",
        "                    save_model(pipeline)\n",
        "            else:\n",
        "                st.error(\"No labels to train on ‚Äî please upload labeled CSV or train manually.\")\n",
        "                pipeline = None\n",
        "\n",
        "        if pipeline is not None:\n",
        "            with st.spinner(\"Predicting...\"):\n",
        "                preds, probs = predict_df(pipeline, texts_clean)\n",
        "                df_out = df.copy()\n",
        "                df_out[\"predicted_label\"] = preds\n",
        "                if probs is not None:\n",
        "                    # Add probability columns\n",
        "                    prob_df = pd.DataFrame(probs, columns=[f\"prob_{c}\" for c in pipeline.classes_])\n",
        "                    df_out = pd.concat([df_out.reset_index(drop=True), prob_df.reset_index(drop=True)], axis=1)\n",
        "                csv = df_out.to_csv(index=False).encode('utf-8')\n",
        "                st.download_button(\"Download predictions CSV\", data=csv, file_name=\"predictions.csv\", mime=\"text/csv\")\n",
        "                st.success(\"Predictions ready and downloadable.\")\n",
        "\n",
        "    # Advanced: DistilBERT fine-tuning\n",
        "    if use_distilbert:\n",
        "        st.subheader(\"DistilBERT Fine-tuning (Advanced)\")\n",
        "        st.markdown(\"\"\"\n",
        "            **Notes:**\n",
        "            Fine-tuning requires 'transformers', 'datasets', and ideally a GPU.\n",
        "            This action can take a long time on CPU and may fail without enough memory.\n",
        "            Use only on medium-sized labeled datasets (thousands of rows) and when comfortable.\n",
        "        \"\"\")\n",
        "        if not has_labels:\n",
        "            st.warning(\"DistilBERT fine-tuning requires labeled data. Upload a labeled CSV and try again.\")\n",
        "        else:\n",
        "            do_finetune = st.button(\"Start DistilBERT fine-tuning (this will run here!)\")\n",
        "            if do_finetune:\n",
        "                try:\n",
        "                    from datasets import Dataset\n",
        "                    from transformers import (\n",
        "                        AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "                    )\n",
        "                    # Prepare a small HF dataset\n",
        "                    st.info(\"Preparing dataset for transformers...\")\n",
        "                    # Map labels to numeric\n",
        "                    unique_labels = y.unique().tolist()\n",
        "                    label2id = {lab: i for i, lab in enumerate(unique_labels)}\n",
        "                    y_numeric = y.map(label2id)\n",
        "                    ds = Dataset.from_pandas(pd.DataFrame({\"text\": texts_raw, \"label\": y_numeric}))\n",
        "                    ds = ds.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "                    model_name = \"distilbert-base-uncased\"\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                    def tokenize_fn(batch):\n",
        "                        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "                    ds = ds.map(tokenize_fn, batched=True)\n",
        "                    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "                    # Model\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
        "\n",
        "                    training_args = TrainingArguments(\n",
        "                        output_dir=\"./distilbert_sentiment\",\n",
        "                        evaluation_strategy=\"epoch\",\n",
        "                        save_strategy=\"epoch\",\n",
        "                        learning_rate=bert_lr,\n",
        "                        per_device_train_batch_size=bert_batch,\n",
        "                        per_device_eval_batch_size=bert_batch,\n",
        "                        num_train_epochs=bert_epochs,\n",
        "                        weight_decay=0.01,\n",
        "                        logging_dir=\"./logs\",\n",
        "                        load_best_model_at_end=True,\n",
        "                    )\n",
        "\n",
        "                    def compute_metrics_eval(eval_pred):\n",
        "                        logits, labels = eval_pred\n",
        "                        preds = np.argmax(logits, axis=-1)\n",
        "                        return {\n",
        "                            \"accuracy\": (preds == labels).mean()\n",
        "                        }\n",
        "\n",
        "                    trainer = Trainer(\n",
        "                        model=model,\n",
        "                        args=training_args,\n",
        "                        train_dataset=ds['train'],\n",
        "                        eval_dataset=ds['test'],\n",
        "                        compute_metrics=compute_metrics_eval\n",
        "                    )\n",
        "                    st.info(\"Training started (this may take a long time). Outputs will be saved to ./distilbert_sentiment\")\n",
        "                    trainer.train()\n",
        "                    st.success(\"Fine-tuning finished. Model saved to ./distilbert_sentiment\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"DistilBERT training failed or libraries missing: {e}\")\n",
        "                    st.info(\"Ensure `transformers`, `datasets`, and `torch` are installed and you have sufficient RAM/GPU.\")\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Built with ‚ù§Ô∏è  contact me if you'd like a custom version for your business Regard Taimour_AI.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd2e6744",
        "outputId": "741f3af3-8d48-4ffe-9ed5-fe4a9f2ff585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m142.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n"
          ]
        }
      ],
      "source": [
        "%pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6614vyD-RoW2",
        "outputId": "01ac27de-1038-439b-fc68-2c946caaaea8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.99.62:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNSAAeiFRrn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}